<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title type="text">stephenpanaro.com</title>

    <updated>2023-11-16T0:05:00Z</updated>
    <id>https://www.stephenpanaro.com/feed.xml</id>
    <link rel="alternate" type="text/html"
        hreflang="en" href="https://www.stephenpanaro.com" />
    <link rel="self" type="application/atom+xml"
        href="https://www.stephenpanaro.com/feed.xml" />
    <rights>Stephen Panaro © 2023-Now</rights>
    <generator uri="https://www.stephenpanaro.com" version="1.0">
        Me
    </generator>

    <entry>
        <title type="text">Inside Apple's 2023 Transformer Models</title>
        <link rel="alternate" type="text/html"
            href="https://www.stephenpanaro.com/blog/inside-apples-2023-transformers" />
        <id>tag:stephenpanaro.com,2023-11-16:/blog/inside-apples-2023-transformers</id>
        <updated>2023-11-16T12:00:00Z</updated>
        <published>2023-11-16T12:00:00Z</published>
        <author>
            <name>Stephen Panaro</name>
        </author>
        <content type="html" xml:lang="en">
            <![CDATA[

        <span class="subhead">What can we learn from them?</span>
        <p>Apple's latest OSes include several transformer models that are optimized for the Apple Neural Engine. We'll
            take a look at how they're implemented and see if there's anything we can apply to our own models. To make
            that easier, I've cobbled together support for viewing them in Netron&mdash;you can try it yourself <a
                href="https://github.com/smpanaro/netron/tree/espresso-mil">here</a>.</p>
        <hr />
        <p>While everyone is talking about AI or GPT, Apple made a point to use the words "machine learning" and
            "transformer" when announcing new features for this year's operating systems (iOS 17 and macOS Sonoma).</p>
        <p>Apple has been vocal about their Machine Learning accelerator, the Neural Engine (ANE), so it's no surprise
            that these models are designed to leverage its capabilities.</p>
        <p>In contrast to their normal secrecy, Apple has been fairly public about how to run the transformer model
            architecture on the ANE. In the past year and a half they:</p>
        <ul>
            <li>Wrote a <a href="https://machinelearning.apple.com/research/neural-engine-transformers">research
                    article</a> about how to optimize transformers for the ANE.<ul>
                    <li>Released code to demonstrate it in the <a
                            href="https://github.com/apple/ml-ane-transformers">ml-ane-transformers repo</a>.</li>
                </ul>
            </li>
            <li>Published a Stable Diffusion (text to image) implementation optimized for the ANE in the <a
                    href="https://github.com/apple/ml-stable-diffusion">ml-stable-diffusion repo</a>.<ul>
                    <li>They have kept this up to date too!</li>
                </ul>
            </li>
        </ul>
        <p>The models embedded in the new OS are not quite as easily inspected as a research article or GitHub project.
            However they are a year newer. Let's see what we can learn from them!</p>
        <blockquote>
            <p>This is most interesting if you're familiar with transformers and how they work. However if you are just
                generally curious I've tried to add explainers throughout to fill in some background.
            <details>
                <summary>They'll look like this.</summary>Feel free to skip them.
            </details>
            </p>
        </blockquote>
        <h2 id="themodels">The Models</h2>
        <p>We'll look at two models today. One powers the keyboard autocomplete, and the other does speech to text. Both
            use the transformer architecture to a degree.</p>
        <p>
        <details>
            <summary>What is a transformer?</summary>
            Transformer is an ML model architecture. This is a specific sequence of mathematical operations that the
            model performs to generate a set of numeric outputs from a given set of inputs. Transformers are
            particularly good at generating text since they predict new words based on all the prior words. They can
            also be used for non-text problems too.
        </details>
        </p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/autocomplete-overview.png"
                alt="annotated image of Netron showing the first layer of the autocomplete transformer model">
            <sub class="block-center image-caption" style="text-align: center;">The input and first layer of the
                autocomplete model, annotated.</sub>
        </p>
        <p>We won't go too deep into the models individually, rather just highlight the interesting bits.</p>
        <h2 id="thevocabsize">The Vocab Size</h2>
        <p><strong>Model:</strong> Keyboard Autocomplete</p>
        <p>The outputs of a transformer are a bunch of probabilities for which token out of the vocab should come next.
            To compute these, you need to load a large mapping from token ID to embedding vector into memory.</p>
        <p>
        <details>
            <summary>Vocab? Probabilities?</summary>
            Transformers operate on numbers, so we need a way to translate between text and numbers. We do this by
            generating a set of pieces of words (and some whole words). Each word piece (aka token) is assigned a
            number, the token ID, that represents it. The group of all word pieces is the vocabulary. The outputs of a
            text generation model is a probability for every token in the vocabulary that is the likelihood it is the
            next token in the sequence.
        </details>
        </p>
        <p>One dimension of this mapping matrix is equal to the number of tokens in the vocabulary. For many models this
            is quite large. gpt-2 (2019) has 50,257 tokens in its vocabulary. LLaMa and Llama2 (2023) have 32,000.</p>
        <p>Apple's autocomplete model only has 15,000. Not only is this number smaller, it is also just underneath the
            Neural Engine's threshold for tensor size. This means that the final computation to determine probabilities
            can happen on the Neural Engine instead of paying the cost to transfer to CPU.</p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/output-logits.png"
                alt="annotated Netron screenshot showing the autocomplete models outputs and indicating that the last inner_product can run on the ANE">
            <sub class="block-center image-caption" style="text-align: center;">The inner_product here is the language
                modeling (lm) head.</sub>
        </p>
        <p><strong>Lesson:</strong> If possible, keep your vocab under 16384. <sup>[1]<sup></p>
        <p><sub>[1] If you don't have control of this, you can duplicate the embedding matrix and do most of the
                computation on ANE. <a
                    href="https://github.com/RobertRiachi/ANE-Optimized-Whisper-OpenAI/blob/d42252155b8e29b2e2c32e7b911ec647198547fb/model.py#L181-L183">Here's
                    an example</a>.</sub></p>
        <h2 id="thekvcache">The KV Cache</h2>
        <p><strong>Model:</strong> Speech to Text</p>
        <p>When using transformers for text generation, a common way to speed them up is to use KV caching. This saves
            you a decent amount of computation.</p>
        <p>
        <details>
            <summary>What is KV Caching?</summary>
            A central part of the transformer architecture is multiplying 3 matrices together. They are the Query, Key
            and Value matrices. An interesting aspect about repeatedly generating text with transformers is that the
            contents of these matrices is mostly the same from prediction to prediction. This means we can avoid a bunch
            of computation by reusing the K and V matrices from the last token we predicted. These are the KV cache.
        </details>
        </p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/traditional-kv-cache.png"
                alt="visualization of the Q and K multiplication without a cache and with a traditional single-token-Q cache">
            <sub class="block-center image-caption" style="text-align: center;">An example of how the Key (K) cache is
                used. With traditional KV caching, the input is 1 token and the cache is the size of all past
                tokens.</sub>
        </p>
        <p>In most implementations, the size of the KV cache increments for each new token. The ANE requires that a
            model's inputs and outputs are a fixed size<sup>*</sup>, which means a traditional KV cache is off the
            table.
            <br /><sub>*not strictly true, but practically</sub>
        </p>
        <p>You can use KV caching for any transformer model, not just text generation, and it seems that Apple has found
            a way to make it work for their speech-to-text model.</p>
        <p>They have side-stepped the ANE constraints by using a fixed size input for their new tokens and sliding their
            KV cache by that same amount for each inference.</p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/apple-kv-cache.png"
                alt="visualization of two back-to-back inferences using Apple's sliding KV cache">
            <sub class="block-center image-caption" style="text-align: center;">Apple's KV cache slides so that the
                inputs are always the same size. In this example there are always 2 input tokens and cache that encodes
                3 tokens. This gives an effective sequence length of 5.</sub>
        </p>
        <p>This gives a meaningful speed up (2-5x in my experience). However there are two caveats.</p>
        <p>First, you have to use <a
                href="https://developer.apple.com/documentation/coreml/mlmultiarray/3882834-initwithpixelbuffer?language=objc">IOSurface-backed</a>
            inputs and outputs otherwise all of the speed gained is lost again by time spent copying them in and out of
            CoreML. Second, if you are on Sonoma/iOS17, you can't have any CPU segments at the start of your model or it
            will be really slow&mdash;this seems like a regression so I have filed feedback.</p>
        <p><strong>Lesson:</strong> Use KV caching. If you're on Sonoma/iOS17, do your CPU work in a separate model.</p>
        <h3 id="bonusthekeycache">Bonus: The Key Cache</h3>
        <p>The KV cache is actually a concatenation of caches for two different tensors: a Key (K) and Value (V). Often
            these are combined into one cache for simplicity, but Apple keeps them separate.</p>
        <p>Why keep them separate? First, you can store the Key cache transposed instead of transposing it before using
            it. Transposing large tensors is extra work that you can avoid (this is in line with Apple's principle of
            "minimize memory copies"). Secondly, the KV cache is a large tensor and by separating it into two, you keep
            the intermediate tensors smaller.</p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/transposed-key-cache.png"
                alt="screenshot of netron showing separate K and V caches, and that the K cache is transposed">
            <sub class="block-center image-caption" style="text-align: center;">Separate caches for K and V and K is
                transposed.</sub>
        </p>
        <p>I don't see much impact from this, but it makes sense to me since you are avoiding work.</p>
        <p><strong>Lesson:</strong> Maybe transpose your K cache and keep it separate from the V cache.</p>
        <h2 id="customlayernorm">Custom Layer Norm</h2>
        <p><strong>Model:</strong> Both</p>
        <p>
        <details>
            <summary>What is a layer norm?</summary>
            Layer Norm is one of the operations that a transformer model uses. It scales the values of a tensor so they
            have certain statistical properties. Layer norm does this along a particular axis of the tensor.
        </details>
        </p>
        <p>One of the optimizations Apple recommends for the Neural Engine is to use a layer norm that normalizes along
            an uncommonly used axis. PyTorch's layer norm doesn't support this, so Apple provides a multi-step manual
            implementation.</p>
        <p>
        <details>
            <summary>Why does it matter what PyTorch supports?</summary>
            In order to run models on Apple's devices, they need to be converted to CoreML. The easiest way to convert
            them is by starting from a PyTorch (a Python ML framework) model. So if you want something, PyTorch needs to
            support it. <sup>There are other ways but they are more complex.</sup>
        </details>
        </p>
        <p>I was curious to see what Apple used for the layer norm for two reasons. First, on Ventura/iOS 16 I found
            that the layer_norm (specifically the reduce_mean) caused my models to lose precision in float16. Second,
            CoreML has native support for layer norm along the uncommon axis and I was curious if it would be used.</p>
        <p>Interestingly enough, it seems like Apple uses the same implementation that they open sourced in
            ml-ane-transformers. You can even see that most of the variable names line up!</p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/layer-norm-comparison.png"
                alt="side-by-side of ml-ane-transformers layer_norm.py and netron of the layer norm with arrows pointing to the commonalities">
            <sub class="block-center image-caption" style="text-align: center;">Almost exactly the same! I am slightly
                confused by the alpha in the zero_mean though.</sub>
        </p>
        <p>I was hoping for something creative here, but on the plus side it seems that layer norm is more resilient in
            float16 on the new OSes.</p>
        <p><strong>Lesson:</strong> Just use Apple's custom layer norm.</p>
        <h2 id="quantization">Quantization</h2>
        <p><strong>Model:</strong> Both</p>
        <p>Both models use quantization to reduce the size of their weight parameters. Transformer models are often
            bottlenecked by the amount of weight parameters they have to load and then unload. The new OSes have support
            for runtime de-quantization which helps reduce this bottleneck.</p>
        <p>This can reduce the accuracy of your model, so keep an eye on that.</p>
        <p><strong>Lesson:</strong> Try quantizing your model. Two good sources: <a
                href="https://apple.github.io/coremltools/docs-guides/source/optimization-overview.html">coremltools
                docs</a> and this Huggingface/ml-stable-diffusion <a
                href="https://huggingface.co/blog/stable-diffusion-xl-coreml#what-is-mixed-bit-palettization">article</a>.
        </p>
        <h2 id="otherobservations">Other Observations</h2>
        <p>There are a couple other things I noticed but I don't know how to take advantage of them. Despite that, they
            are still interesting in and of themselves&mdash;if you see a way to use them, please let me know!</p>
        <p><strong>Single Input</strong> The text autocomplete model takes 3 inputs: 128 token IDs, 128 position values
            and 128 segment values. It passes them to the model as one concatenated input and then immediately splits
            them. I'm not sure the benefit of this, but it seems slightly odd so maybe there is one?</p>
        <p><img src="https://www.stephenpanaro.com/static/blog/inside-apples-2023-transformers/input-embeddings.png"
                alt="the input embeddings of the autocomplete model in netron" />
            <sub class="block-center image-caption" style="text-align: center;">In the autocomplete model, the 3
                embedding fields are passed as one input.</sub>
        </p>
        <p><strong>Shared Weights</strong> The text autocomplete model actually has two versions, one for CPU and one
            for ANE. They are slightly different (different inputs and outputs), but they both share the same weights. I
            don't believe this is currently possible using Apple's provided tooling, but it does open up some
            interesting possibilities. To achieve something similar today you have to ship two copies of the same
            weights.</p>
        <p><code>
$ head -n2 unilm_joint_ane.espresso.net
<br />
&lcub;
  "storage": "unilm_joint.espresso.weights",
<br />
$ head -n2 unilm_joint_cpu.espresso.net
<br />
&lcub;
  "storage": "unilm_joint.espresso.weights",
</code></p>
        <p><strong>MultiHead Softmax</strong> Apple's implementation of the transformer in ml-ane-transformers splits a
            large matrix multiplication up into several smaller ones, then performs a softmax on each result (<a
                href="https://github.com/apple/ml-ane-transformers/blob/da64000fa56cc85b0859bc17cb16a3d753b8304a/ane_transformers/reference/multihead_attention.py#L80-L108">here</a>).
            In contrast, the autocomplete model concatenates the results of the split matrix multiplications, performs
            one softmax, then re-splits that. I didn't see any performance difference from doing this, but I was only
            looking at speed.</p>
        <p><strong>Extra Outputs</strong> The CPU version of the autocomplete model outputs the next token logits, but
            also the pre-logit embeddings. This isn't super novel, but worth mentioning since the cost of getting
            already-existing data out of the model seems to be fairly low if you use IOSurface-backed buffers as
            mentioned above. This might be counterintuitive since some of these outputs can be rather large.</p>
        <h2 id="seeforyourself">See for Yourself</h2>
        <p>Those are the eight things that stood out to me from looking at Apple's new models. Four of them are useful,
            four of them are just interesting.</p>
        <p>If you'd like to look for yourself, you can find the models here on macOS Sonoma:</p>
        <ul>
            <li>Autocomplete: <code
                    class="long-code">/System/Library/LinguisticData/RequiredAssets_en.bundle/AssetData/en.lm/unilm.bundle</code>
            </li>
            <li>Speech to Text: <code
                    class="long-code">find /System/Library/AssetsV2/com_apple_MobileAsset_Trial_Siri_SiriUnderstandingAsrAssistant -name "AM-Conformer"</code>
            </li>
        </ul>
        <p>I have a hacky <a href="https://github.com/smpanaro/netron/tree/espresso-mil">fork</a> of Netron here that
            can open them (it will only open the first 3000 operations of the Speech to Text model since it is huge).
        </p>
        <p>If you find anything interesting or if I misinterpreted something I would love to know. Drop me a line!</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title type="text">No Frills Time Series Compression That Also Works</title>
        <link rel="alternate" type="text/html"
            href="https://www.stephenpanaro.com/blog/time-series-compression" />
        <id>tag:stephenpanaro.com,2023-08-22:/blog/time-series-compression</id>
        <updated>2023-08-22T12:00:00Z</updated>
        <published>2023-08-22T12:00:00Z</published>
        <author>
            <name>Stephen Panaro</name>
        </author>
        <content type="html" xml:lang="en">
            <![CDATA[
            <span class="subhead">CSV + gzip will take you far.</span>

            <p>
                So you have some time series data and you want to make it smaller?
                You may not need an algorithm designed specifically for time series.
                Generic compressors like gzip work quite well and are much easier to
                use.
            </p>
            <p>
                Of course this depends on your data, so there’s some code you can
                use to try it out <a href="https://github.com/smpanaro/time-series-compression">here</a>.
            </p>

            <hr />

            <p>
                Recently I started working on a way to save Bluetooth scale data in
                my iOS coffee-brewing app. I want to allow people to record from a
                scale during their coffee-brewing sessions and then view it
                afterwards. Scale data is just a bunch of timestamps and weight
                values. Simple, yes, but it felt like something that might take a
                surprising amount of space to save. So I did some napkin math:
            </p>

            <div class="sequence">
                <code style="white-space: nowrap;">
                    1 scale session / day
                    <br />
                    10 minutes / session
                    <br />
                    10 readings / second
                    <br />
                    = 2.19M readings / year
                </code>
                <br />
                <code style="white-space: nowrap;">
                    1 reading = 1 date + 1 weight
                    <br />
                    = 1 uint64 + 1 float32
                    <br />
                    = 12 bytes
                    <br />
                    2.19M * 12B = 26 MB
                </code>
            </div>

            <p>
                26 MB per year is small by most measures. However in my case I keep
                a few extra copies of my app’s data around as backups so this is
                more like ~100MB/year. It’s also 40x the size of what I’m saving
                currently! This puts my app in danger of landing on the one Top Apps
                list I would not be stoked to be featured on:
            </p>


            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/icloud-settings.jpg"
                style="border-radius: 4px;" alt="iCloud storage usage in iOS Settings app" />
            <sub class="block-center image-caption" style="text-align: center;">iCloud storage usage</sub>

            <p>
                So let’s avoid that. At a high-level I see two options:
            </p>

            <p>
                <b>Save less.</b> 10 scale readings/second is probably more
                granularity than we’ll ever need. So we could just not save some of
                them. Of course if I’m wrong about that, they’re gone forever and
                then we’ll be out of luck.
            </p>

            <p>
                <b>Save smaller.</b> Looking at some example data, there are a lot
                of plateaus where the same value repeats over and over. That seems
                like it could compress well.
            </p>

            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/session-graph.jpg"
                style="border-radius: 10px;" alt="Example brewing session time series chart" />
            <sub class="block-center image-caption" style="text-align: center;">Example brewing session time series</sub>

            <h2>Picking Ways to Compress</h2>
            <p>
                This is my first rodeo with compression. I’m starting from basics
                like “compression makes big things small” and “double click to
                unzip”. Doing a little research seems like a good idea and it pays
                off.
            </p>

            <p>
                My scale data is technically “time series data” and it turns out we
                are not the first to want to compress it. There is a whole family
                of algorithms designed specifically for time series. This <a
                    href="https://www.timescale.com/blog/time-series-compression-algorithms-explained/">blog post</a>
                is a great deep dive, but for our purposes today we’ll be looking at
                two of the algorithms it mentions:
            </p>
            <ul>
                <li><i>simple-8b</i> which compresses sequences of integers</li>
                <li><i>Gorilla</i> which compresses both integers as well as floating point numbers</li>
            </ul>

            <p>
                Algorithms designed for exactly my problem space sound ideal.
                However something else catches my eye in a <a
                    href="https://news.ycombinator.com/item?id=31385515">comment</a> about the same
                blog post:
            </p>

            <blockquote>
                <div style="opacity: 0.5;">rklaehn on May 15, 2022</div>
                I have found that a very good approach is to apply some very simple
                transformations such as delta encoding of timestamps, and then
                letting a good standard compression algorithm such as zstd or
                deflate take care of the rest.
            </blockquote>

            <p>
                Using a general purpose algorithm is quite intriguing! One thing
                I’ve noticed is that there are no Swift implementations for
                simple-8b or Gorilla. This means I would have to wrap an existing
                implementation (a real hassle) or write a Swift one (risky, I would
                probably mess it up). General purpose algorithms are much more
                common and side-step both of those issues.
            </p>

            <p>
                So we’ll look at both. For simplicity I’ll call simple-8b and Gorilla the “specialist algorithms” and
                everything
                else “generalist”.
            </p>

            <h2>Evaluating the Specialist Algorithms</h2>
            <p>
                Starting with the specialists seems logical. I expect they will
                perform better which will give us a nice baseline for comparison.
                But first we need to smooth out a few wrinkles.
            </p>

            <h3>Precision</h3>

            <p>
                While wiring up an open-source simple-8b implementation I realize that
                it requires integers and both our timestamp and weight are floating
                point numbers. To solve this we’ll truncate to milliseconds and
                milligrams. A honey bee can flap its wings in 5 ms. A grain of salt is
                approximately 1mg. Both of these feel way more precise than necessary
                but better to err on that side anyways.
            </p>

            <div class="sequence">
                <code>
                    49.0335097 seconds
                    <br />
                    17.509999999999998 grams
                </code>
                <br />
                <code>
                    49033 milliseconds
                    <br />
                    17509 milligrams
                </code>
            </div>

            <p>
                We’ll use this level of precision for all our tests except Gorilla, which is designed for
                floating point
                numbers.
            </p>

            <h3>Negative Numbers</h3>
            <p>
                Negative numbers show up semi-frequently in scale data because often when
                you pick something up off a scale it will drop below zero.
            </p>

            <p>
                Unfortunately for us simple-8b doesn’t like negative numbers. Why?
                Let’s take a little detour and look at how computers store numbers.
                They end up as sequences of 1s and 0s like:
            </p>

            <code style="display: block; margin: 30px 0;">
                0000000000010110 is 22
                <br />
                0000000001111011 is 123
                <br />
                0000000101011110 is 350
            </code>

            <p>
                You’ll notice that these tend to have all their 1s all on the right.
                In fact, only very large numbers will have 1s on the left. simple-8b
                does something clever where it uses 4 of the leftmost spaces to
                store some 1s and 0s of its own. This is fine for us. We’re not
                storing huge numbers so those leftmost spaces will always be 0 in
                our data.
            </p>

            <p>
                Now let’s look at some negatives.
            </p>

            <code style="display: block; margin: 30px 0;">
                1111111111101010 is -22
                <br />
                1111111110000101 is -123
                <br />
                1111111010100010 is -350
            </code>

            <p>
                This is not great, the left half is all 1s! Simple-8b has no way of
                knowing whether the leftmost 1 is something it put there or
                something we put there so it will refuse to even try to compress
                these.
            </p>

            <p>
                One solution for this is something called ZigZag encoding. If you
                look at the first few positive numbers, normally they’ll look like
                this:
            </p>

            <code style="display: block; margin: 30px 0;">
                0000000000000001 is 1
                <br />
                0000000000000010 is 2
                <br />
                0000000000000011 is 3
                <br />
                0000000000000100 is 4
            </code>

            <p>
                ZigZag encoding interleaves the negative numbers in between so now
                these same 0/1 sequences take on a new meaning and zig zag between
                negative and positive:
            </p>

            <code style="white-space: pre-line; display: block; margin: 30px 0;">
                0000000000000001 is -1 <i>zig</i>
                0000000000000010 is &nbsp;1 <i>zag</i>
                0000000000000011 is -2 <i>zig</i>
                0000000000000100 is &nbsp;2 <i>zag</i>
            </code>

            <p>
                If we look at our negative numbers from earlier, we can see that
                this gets rid of our problematic left-side 1s.
            </p>

            <div style="overflow: auto;">
                <table>
                    <tr style="text-align: left; font-size: 14px;">
                        <th>#</th>
                        <th style="padding: 0 16px; border-left: 1px dashed;">Normal</th>
                        <th style="padding-left: 16px; border-left: 1px dashed;">
                            ZigZag
                        </th>
                    </tr>
                    <tr>
                        <td style="padding-right: 16px;">
                            <code>
                        -22
                        <br />
                        -123
                        <br />
                        -350
                        </code>
                        </td>
                        <td style="padding: 0 16px; border-left: 1px dashed;">
                            <code>
                        1111111111101010
                        <br />
                        1111111110000101
                        <br />
                        1111111010100010
                        </code>
                        </td>
                        <td style="padding-left: 16px; border-left: 1px dashed;">
                            <code>
                        <u>0000000000</u>101011
                        <br />
                        <u>00000000</u>11110101
                        <br />
                        <u>000000</u>1010111011
                        </code>
                        </td>
                    </tr>
                </table>
            </div>

            <p>
                We only need this for simple-8b, but it can be used with other
                integer encodings too. Kinda cool!
            </p>

            <h3>Pre-Compression</h3>
            <p>
                Technically we could run our tests now, but we’re going to do two
                more things to eke out a little extra shrinkage.
            </p>

            <p>
                First is delta encoding. The concept is simple: you replace each
                number in your data set with the difference (delta) from the
                previous value.
            </p>

            <div class="sequence">
                <code>
                    timestamp,mass
                    <br />
                    1691452800000,250
                    <br />
                    1691452800103,253
                    <br />
                    1691452800305,279
                    <br />
                    &hellip;
                    </code>
                <div class="sequence-rotate-90" style="padding: 0 32px; margin: auto 0; border: none;">
                    &rightarrow;
                </div>
                <code style="border: none; padding: 0;">
                    timestamp_delta,mass_delta
                    <br />
                    1691452800000,250
                    <br />
                    103,3
                    <br />
                    202,26
                    <br />
                    &hellip;
                </code>
            </div>

            <p>
                Visually these already look smaller. Amusingly enough they actually
                are smaller. We’ll use this for all algorithms except Gorilla which
                does delta encoding for us.
            </p>

            <p>
                The second tweak relates to the ordering of our data. So far we’ve
                been talking about time series as pairs of (timestamp, mass) points.
                Both specialist algorithms require us to provide a single list of
                numbers. We have two choices to flatten our pairs:
            </p>

            <code style="display: block; margin: 30px 0;">
                <b>Choice 1</b>: [first_timestamp, first_mass, second_timestamp, second_mass, &hellip;]
                <br />
                <b>Choice 2</b>: [first_timestamp, second_timestamp, … last_timestamp, first_mass, second_mass, &hellip;]
            </code>

            <p>
                Choice 2 compresses better on all algorithms (generalist too) even
                when we apply it after delta encoding. Again, Gorilla does its own
                thing–are you seeing the trend?
            </p>

            <h3>Specialist Results</h3>
            <p>
                We’ve truncated and pre-encoded, so let’s see some results.
            </p>

            <div style="overflow: auto;">
                <table class="results-table" style="margin-bottom: 20px;">
                    <thead>
                        <tr>
                            <th>Algorithm</th>
                            <th>Ratio 1</th>
                            <th>Ratio 2</th>
                            <th>Ratio 3</th>
                            <th>Avg. Ratio</th>
                            <th>Avg. MB/year</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>simple-8b</td>
                        <td>6.92</td>
                        <td>5.4</td>
                        <td>7.18</td>
                        <td>6.5</td>
                        <td>4</td>
                    </tr>
                    <tr>
                        <td>gorilla</td>
                        <td>6.72</td>
                        <td>4.18</td>
                        <td>6.88</td>
                        <td>5.9</td>
                        <td>4.4</td>
                    </tr>
                    <tfoot>
                        <tr style="font-size: 12px; opacity: 0.6;">
                            <td></td>
                            <td colspan="4">
                                <div style="display: flex;">
                                    &RightTee; <span style="flex: 1; text-align: center;">higher is better</span>
                                    &LeftTee;
                                </div>
                            </td>
                            <td>
                                <div style="display: flex; line-height: 14px;">
                                    lower is better
                                </div>
                            </td>
                        </tr>
                    </tfoot>
                </table>
            </div>

            <p>
                I tested with three different types of scale recordings for a bit of
                variety, then backed out the MB/year from the average compression
                ratio. Going from 26 MB/year to under 5 is a great result!

            <h2>Now for the Generalist Ones</h2>
            <p>
                Similar to the specialist algorithms, we have a few
                choices to make before we can run our tests on the generalists.
            </p>

            <h3>Formatting</h3>

            <p>
                For simplicity we’re going to format our data as CSV. This might seem a little odd but it has a
                few perks:
            </p>
            <ul>
                <li>It’s human-readable which is nice for debugging.</li>
                <li>It’s also fairly compact as far as text representations go.</li>
                <li>Most languages have native libraries to make reading/writing CSVs easy. <sup
                        style="opacity: 0.6;">(alas,
                        Swift does
                        not)</sup></li>
            </ul>

            <p>
                We’ll use delta encoding like above–it’d be silly not to. We could
                really stretch the definition of CSV and stack all of the timestamps
                on top of all the masses into a single column, but that sacrifices a
                bit of readability so we won’t.
            </p>

            <h3>Picking Algorithms</h3>

            <p>
                There are a lot of general purpose compression algorithms. One
                popular benchmark lists over 70! We’re going to pick just 5. They
                are:
            </p>
            <ul>

                <li>
                    <i>zlib</i>, <i>LZMA</i>, and <i>LZFSE</i> – these come built-in with iOS which makes
                    my life easier. zlib and LZMA are also fairly common.
                </li>
                <li>
                    <i>Zstandard</i> (aka zstd) and <i>Brotli</i> – from Facebook and Google
                    respectively, both companies with an interest in good
                    compression
                </li>
            </ul>

            <h3>Picking Levels</h3>

            <p>
                We’ve narrowed it down from 70 to 5, but there’s another curveball.
                Unlike the specialist algorithms which have no configuration
                options, most generalist algorithms let you choose a level that
                trades off speed for better compression. You can compress fast or
                slow down to compress more.
            </p>

            <p>
                For simplicity (and so I don’t have to show you a table with 40+
                rows) we are not going to test all 11 Brotli levels or all 20+ zstd
                levels. Instead we’re going to choose levels that run at about the
                same speed. Apple makes this easier for us since LZFSE has no level
                and iOS only has zlib 5 and LZMA 6. All we have to do is pick levels
                for Brotli and zstd from this chart.
            </p>

            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/speed-chart.png" style="border-radius: 4px;"
                alt="Chart of speed benchmarks for our 5 algorithms at various levels" />
            <sub class="block-center image-caption" style="text-align: center;">Speed benchmarks for our 5
                algorithms</sub>

            <p>
                We’ll use Brotli 4 and zstd 5 since those are in-line with the
                fastest iOS algorithm. This means that zlib and LZMA are slightly
                advantaged but we’ll keep that in mind.
            </p>

            <h3>Generalist Results</h3>

            <p>
                We’ve prepped our CSV and made all our choices, so let’s see some results.
            </p>

            <div style="overflow: auto;">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Algorithm</th>
                            <th>Ratio 1</th>
                            <th>Ratio 2</th>
                            <th>Ratio 3</th>
                            <th>Avg. Ratio</th>
                            <th>Avg. MB/year</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>zlib 5</td>
                        <td>8.50</td>
                        <td>5.79</td>
                        <td>8.18</td>
                        <td>7.49</td>
                        <td>3.47</td>
                    </tr>
                    <tr>
                        <td>lzma 6</td>
                        <td>8.12</td>
                        <td>5.55</td>
                        <td>7.49</td>
                        <td>7.1</td>
                        <td>3.7</td>
                    </tr>
                    <tr>
                        <td>zstd 5</td>
                        <td>7.49</td>
                        <td>5.71</td>
                        <td>7.74</td>
                        <td>6.98</td>
                        <td>3.72</td>
                    </tr>
                    <tr>
                        <td>brotli 4</td>
                        <td>7.84</td>
                        <td>5.52</td>
                        <td>7.53</td>
                        <td>6.96</td>
                        <td>3.74</td>
                    </tr>
                    <tr>
                        <td>lzfse</td>
                        <td>7.49</td>
                        <td>5.36</td>
                        <td>7.12</td>
                        <td>6.7</td>
                        <td>3.8</td>
                    </tr>
                    <tfoot>
                        <tr style="font-size: 12px; opacity: 0.6;">
                            <td></td>
                            <td colspan="4">
                                <div style="display: flex;">
                                    &RightTee; <span style="flex: 1; text-align: center;">higher is better</span>
                                    &LeftTee;
                                </div>
                            </td>
                            <td>
                                <div style="display: flex; line-height: 14px;">
                                    lower is better
                                </div>
                            </td>
                        </tr>
                    </tfoot>
                </table>
            </div>

            <!--
                Split CSV
                algorithm
                ratio1
                ratio2
                ratio3
                avg
                Avg MB/year
                zlib 5
                9.61
                6.56
                8.98
                8.38
                3.1
                lzma 6
                9.31
                6.31
                8.40
                8
                3.25
                zstd 5
                10.70
                6.95
                9.61
                9.09
                2.86
                brotli 4
                10.47
                6.87
                9.57
                8.97
                2.9
                lzfse
                8.80
                6.05
                8.18
                7.67
                3.39
                -->


            <p>

                Wow! Everything is under 4MB. Coming from 26MB this is fantastic.
            </p>

            <h2>Specialist v. Generalist</h2>

            <p>
                I’ve plotted everything side-by-side:
            </p>

            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/compression-chart-2.png"
                style="border-radius: 4px;" alt="Chart of MB/year by algorithm" />
            <sub class="block-center image-caption" style="text-align: center;">MB/year by algorithm</sub>

            <p>
                Weirdly, the generalist algorithms universally beat the specialists.
                On top of that, you’ll recall we picked generalist levels that were
                fairly fast. So we can actually widen the gap if we’re willing to
                compress slower.
            </p>

            <p>
                That feels like cheating, but doing the single column CSV doesn’t.
                Plus I’m really curious about that, so here it is:
            </p>

            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/compression-chart-3.png"
                style="border-radius: 4px;" alt="Chart of MB/year by algorithm including single column CSV results" />
            <sub class="block-center image-caption" style="text-align: center;">MB/year by algorithm including single
                column
                CSV
                results</sub>

            <p>
                Seems like if you’re not a CSV purist you can squeeze an extra 400KB or so. Not bad.
            </p>

            <h2>What Gives?</h2>

            <p>
                It really does not make sense to me that the generalist algorithms come out on top.
            </p>

            <p>
                It’s possible I made a mistake somewhere. To check this, I look to
                see if every compressed time series can be reversed back to the
                original scale time series. They all can.
            </p>

            <p>
                My second guess is that maybe my time series data is not well-suited
                for simple-8b and Gorilla. I saw mention that equally spaced
                timestamps are preferred and my data is anything but:
            </p>


            <table>
                <tr>
                    <td>
                        <code>timestamps</code>
                    </td>
                    <td style="padding-left: 16px;">
                        <code>deltas</code>
                    </td>
                </tr>
                <tr>
                    <td style="padding-right: 16px;">
                        <code>
                            <span style="opacity: 0.4;">1691685057</span>323
                            <br />
                            <span style="opacity: 0.4;">1691685057</span>413
                            <br />
                            <span style="opacity: 0.4;">1691685057</span>504
                            <br />
                            <span style="opacity: 0.4;">1691685057</span>622
                            <br />
                            <span style="opacity: 0.4;">1691685057</span>732
                        </code>
                    </td>
                    <td style="padding-left: 16px; border-left: 1px dashed;">
                        <code>
                            n/a
                            <br />
                            90
                            <br />
                            91
                            <br />
                            118
                            <br />
                            110
                        </code>
                    </td>
                </tr>
            </table>

            <p>
                To see if this is the problem, I re-run the benchmarks and truncate
                timestamps to the nearest 0.01s, 0.1s and even 1s. This ensures that
                there is a finite sized set of delta values (101, 11 and 2
                respectively).
            </p>

            <img class="block-center" src="https://www.stephenpanaro.com/static/blog/time-series-compression/granularity-chart.png"
                style="border-radius: 4px;" alt="Chart of compression ratio by timestamp granularity" />
            <sub class="block-center image-caption" style="text-align: center;">Compression ratio by timestamp
                granularity</sub>

            <!--
            algo
            0.001s (original)
            0.01s
            0.1s
            1s
            simple-8b
            6.92
            9.60
            14.07
            14.15
            gorilla
            6.72
            7.21
            11.39
            12.26
            lzfse
            8.80
            12.48
            17.69
            26.76
            -->

            <p>


                As expected this does improve the compression ratio of the specialist algorithms. But it also
                gives a
                similar
                boost to the generalist one. So it doesn’t explain the difference.
            </p>

            <p>

                I don’t have a third guess. Maybe it is real?
            </p>

            <h2>Back to Where We Started</h2>

            <p>
                This all started since I was anxious about inflating the size of my
                humble iOS app. Our baseline was adding 26MB of new data each year,
                which became ~100MB/year in iCloud. With a general purpose
                compression algorithm it looks like we can get these numbers down to
                ~4MB and ~16MB per year respectively. Much better.
            </p>

            <p>
                Any of the generalist algorithms would work. In my case using one of Apple’s built-ins is an
                easy choice:
            </p>
            <ul>
                <li>
                    It’s <a href="https://developer.apple.com/documentation/foundation/nsdata/3174960-compressed">~1
                        line of code</a> to implement them. <sup style="font-size: 12px;
                    opacity: 0.6;">Plus a few lines to make a CSV.</sup>
                </li>
                <li>
                    Using Brotli or zstd would increase my app’s download size by 400-700 KB. Not a lot but avoiding
                    it is nice.
                </li>
            </ul>

            <h2>Try It at Home</h2>

            <p>
                One thing we didn’t touch on is that the distribution of your data
                can impact how well the compression works. It’s possible these
                results won’t translate to your data. To help check that, I’ve put
                my benchmarking CLI tool and a speed-test macOS/iOS app up on GitHub
                <a href="https://github.com/smpanaro/time-series-compression" style="color: cornflowerblue;">here</a>.
            </p>

            <p>
                If you can put your data in CSV format, you should be able to drop
                it in and try out all the algorithms mentioned in this post. If you
                do, let me know what sort of results you get! I'm curious to see
                more real-world data points.
            </p>

            <p>
            Comments or thoughts? Find me on <a href="https://twitter.com/stephenpanaro">twitter</a> or <a
            href="https://mastodon.social/@smpanaro">mastodon</a>.
            </p>
            ]]>
        </content>
    </entry>

</feed>